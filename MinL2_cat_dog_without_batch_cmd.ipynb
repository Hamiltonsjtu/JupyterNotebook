{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import gzip\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取文件并标记编号\n",
    "root_file = 'E:\\\\JupyterNotebook\\\\pic_download_from_web\\\\'\n",
    "filename = []\n",
    "label = []\n",
    "for i in range(len(os.listdir(root_file))):\n",
    "    sub_file_name_dir = os.path.join(root_file, os.listdir(root_file)[i])\n",
    "    for j in range(len(os.listdir(sub_file_name_dir))):\n",
    "        sub_file_name = os.path.join(sub_file_name_dir,os.listdir(sub_file_name_dir)[j])\n",
    "        filename.append(sub_file_name)\n",
    "        label.append(i)\n",
    "filenames = tf.constant(filename)\n",
    "labels = tf.constant(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "######### 1. 使用tf.data.Dataset实现Batch Generator    #############\n",
    "####################################################################\n",
    "# 从硬盘中读取图片\n",
    "def _parse_function(filename, label):\n",
    "    image_string = tf.read_file(filename)\n",
    "    image_decoded = tf.cond(tf.image.is_jpeg(image_string),lambda: tf.image.decode_jpeg(image_string), lambda: tf.image.decode_png(image_string))\n",
    "    image_decoded = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
    "    image_grayed = tf.image.rgb_to_grayscale(image_decoded)\n",
    "    image_resized = tf.image.resize_images(image_grayed, [28, 28], method = 0)\n",
    "    return image_resized, label\n",
    "   \n",
    "# 此时dataset中的一个元素是(filename, label) \n",
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels)) # 此时dataset中的一个元素是(image_resized, label) \n",
    "dataset = dataset.map(_parse_function) # 此时dataset中的一个元素是(image_resized_batch, label_batch) \n",
    "\n",
    "batch_size = 10\n",
    "repeat_num = 10\n",
    "buffer_sieze_num = 100\n",
    "dataset = dataset.shuffle(buffer_size=buffer_sieze_num).batch(batch_size).repeat(repeat_num) # batch size=32, 读入磁盘图片和label,训练时重复10个epoch.\n",
    "\n",
    "# image_pixels = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  实现回归模型\n",
    "x1 = tf.placeholder(tf.float32, shape=([None, 28,28,1]))\n",
    "x = tf.reshape(x1, [-1, 784*1])\n",
    "W = tf.Variable(tf.random_uniform([784*1, 10]))\n",
    "\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "# 训练模型\n",
    "y_ = tf.placeholder('float', [None, 10])\n",
    "cross_entropy = - tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# 初始化模型参数\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_one_shot_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    x_train, y_train = sess.run(next_element)\n",
    "    y_train_onehot = sess.run(tf.one_hot(y_train, 10))\n",
    "    sess.run(train_step, feed_dict={x1: x_train, y_:y_train_onehot })\n",
    "#     while True:\n",
    "#         try: \n",
    "#             x_train, y_train = sess.run(next_element)\n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#             break\n",
    "#     y_train_onehot = sess.run(tf.one_hot(y_train, 10))\n",
    "#     sess.run(train_step, feed_dict={x1: x_train, y_:y_train_onehot })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x_file = os.path.join(data_file, files[3])\n",
    "# train_y_file = os.path.join(data_file, files[1])\n",
    "# test_x_file = os.path.join(data_file, files[2])\n",
    "# test_y_file = os.path.join(data_file, files[0])\n",
    "# # print (test_x_file)\n",
    "# with gzip.open(train_y_file, 'rb') as lbpath:\n",
    "#     y_train = np.frombuffer(lbpath.read(), dtype = np.uint8, offset=8)\n",
    "# with gzip.open(train_x_file, 'rb') as imgpath:\n",
    "#     x_train = np.frombuffer(imgpath.read(), dtype = np.uint8, offset=16).reshape(len(y_train), 784)\n",
    "# with gzip.open(test_y_file, 'rb') as lbpath:\n",
    "#     y_test = np.frombuffer(lbpath.read(), dtype = np.uint8, offset=8)\n",
    "# with gzip.open(test_x_file, 'rb') as imgpath:\n",
    "#     x_test = np.frombuffer(imgpath.read(), dtype = np.uint8, offset=16).reshape(len(y_test),784)\n",
    "# x_train = x_train.astype('float')\n",
    "# y_train = y_train.astype('uint8')\n",
    "# # print ((y_train))\n",
    "# # y_train = tf.one_hot(y_train, 10)\n",
    "# # with tf.Session() as sess:\n",
    "# #     print (sess.run(y_train))\n",
    "# x_test = x_test.astype('float')\n",
    "# y_test = y_test.astype('uint8')\n",
    "\n",
    "# #  实现回归模型\n",
    "# x = tf.placeholder('float', [None, 784])\n",
    "# W = tf.Variable(tf.random_uniform([784, 10]))\n",
    "# b = tf.Variable(tf.zeros([10]))\n",
    "# y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "# # 训练模型\n",
    "# y_ = tf.placeholder('float', [None, 10])\n",
    "# cross_entropy = - tf.reduce_sum(y_*tf.log(y))\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "# init  = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(500):\n",
    "# #     print (\"Step:\" , i+1)\n",
    "#     train_index = np.floor(np.random.uniform(0,1,batch_size)*np.shape(x_train[:,0])).astype(int)\n",
    "#     x_train_batch = x_train[train_index,:]\n",
    "#     # print (y_train)\n",
    "#     y_train_batch = y_train[train_index]\n",
    "# #     print (y_train_batch)\n",
    "#     with tf.Session() as sess:\n",
    "#             sess.run(init)\n",
    "#             output = sess.run(tf.one_hot(y_train_batch, 10))\n",
    "#             sess.run(train_step, feed_dict = {x:x_train_batch, y_:output})\n",
    "\n",
    "# \n",
    "# dataset = tf.data.Dataset.range(100)\n",
    "# next_element = dataset.make_one_shot_iterator().get_next()\n",
    "# with tf.Session() as sess:\n",
    "#     for i in range(100):\n",
    "#         value = sess.run(next_element)\n",
    "#         print ('valeu is %5d' %value)\n",
    "\n",
    "\n",
    "# 可初始化迭代器\n",
    "# max_value = tf.placeholder(tf.int64, shape = [])\n",
    "# dataset = tf.data.Dataset.range(max_value)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "# next_element = iterator.get_next()\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(iterator.initializer, feed_dict={max_value:100})\n",
    "#     for i in range(100):\n",
    "#         value = sess.run(next_element)\n",
    "#         print ('value is %5d' %value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
